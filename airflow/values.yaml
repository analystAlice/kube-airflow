airflow:
  # common settings and setting for the webserver
  # For prefix, use only lower case alphanumeric characters, '-' or '.', and must start and end
  # with an alphanumeric character
  prefix: ""
  fernet_key: ""
  service:
    type: ClusterIP
  dag_path: /dags
  init_retry_loop:
  image: mumoshu/kube-airflow:1.8.0.0-1.6.1
  # image_pull_policy: Always or IfNotPresent
  image_pull_policy: Always
  # Set scheduler_num_runs to control how the schduler behaves:
  #   -1 will let him looping indefinitively but it will never update the DAG
  #   1 will have the scheduler quit after each execution, kubernetes will restart it
  scheduler_num_runs: "1"
  url_prefix: ""
  # airflow-cfg allow you to override the whole content of airflow.cfg at once
  # BEWARE of maintaining uniformity in your configuration, especially since the bootstrap script
  # `entrypoint.sh` will not adapt the various settings accordingly to this configuration, such as
  # rabbitmq/postgres creds/host, git-sync etc.
  # If you choose to hardcode a complete file, just ensure you have the same configuration as
  # defined here (remember the postgres/rabbitmq hostnames are prepended by a prefix defined in
  # `airflow.prefix`). If you use templating you can still look at the example in
  # `values-airflowcfg.yaml`.
  airflow_cfg:
    enable: false
    data:

celery:
  num_workers: 1

ingress:
  enabled: false
  annotations:
    # Define the annotation here to configure rewriting rules related to your Load balancer
    #
    # Please note their is a small difference between the way Airflow Web server and Flower handles
    # url_prefix in HTTP requests:
    #  - airflow webserver handles it completely, just ask your load balancer to give the HTTP
    #    header like the requested URL
    #  - Flower wants HTTP header to behave like there was no URL prefix, and but still form the
    #    right URL in generated pages.
    #
    #    Extracted from the Flower documentation:
    #    (https://github.com/mher/flower/blob/master/docs/config.rst#url_prefix)
    #
    #        To access Flower on http://example.com/flower run it with:
    #            flower --url_prefix=flower
    #
    #        Use the following nginx configuration:
    #            server {
    #              listen 80;
    #              server_name example.com;
    #
    #              location /flower/ {
    #                rewrite ^/flower/(.*)$ /$1 break;
    #                proxy_pass http://example.com:5555;
    #                proxy_set_header Host $host;
    #              }
    #            }
    web:
      # Example for Traefik:
      # traefik.frontend.rule.type: PathPrefix
      # kubernetes.io/ingress.class: traefik
    flower:
      # Example for Traefik:
      # traefik.frontend.rule.type: PathPrefixStrip
      # kubernetes.io/ingress.class: traefik
  host: ""
  path:
    web: /airflow
    flower: /flower

db:
  rabbitmq:
    # use_embedded == true means final hostname will be prefix + basename
    use_embedded: true
    user: airflow
    password: airflow
    basename: rabbitmq
    database: airflow
  postgres:
    # use_embedded == true means final hostname will be prefix + basename
    use_embedded: true
    user: airflow
    password: airflow
    basename: postgres
    database: airflow

persistence:
  enabled: false
  ## storageClass: Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  # storageClass: default
  accessMode: ReadWriteOnce
  size: 1Gi


flower:
  url_prefix: ""

dags:
  # pickle_dag: send DAG using pickle from the scheduler to the worker
  pickle_dag: true
  # Use of git-sync: beware when using git-sync and airflow. If the scheduler reloads a dag in the
  # middle of a dagrun then the dagrun will actually start using the new version of the dag in the
  # middle of execution.
  # This is a known issue with airflow and it means it's unsafe in general to use a git-sync
  # like solution with airflow without:
  # - using explicit locking, ie never pull down a new dag if a dagrun is in progress
  # - make dags immutable, never modify your dag always make a new one
  git_sync_enabled: false
  git_repo:
  git_branch: master
  poll_interval_sec: 60
  git_sync_debug: false
  # Disable Load examples as soon as you enable git_repo
  load_examples: true
